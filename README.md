# counter-deploy

## Цель и исходные данные

Цель работы — реализовать кластеризованное развертывание приложения‑счётчика на базе существующего репозитория, провести сравнительное нагрузочное тестирование и проанализировать последствия масштабирования базы данных. В качестве первого этапа используется Docker Swarm; альтернативный подход рассматривается на уровне Kubernetes (k3s).

## Кластеризованное развертывание в Docker Swarm

Кластеризация выполнена через механизмы Swarm и описана в [docker-compose.yml](docker-compose.yml). Для сервиса `app` задано 4 реплики, что обеспечивает параллельную обработку запросов и балансировку на уровне встроенного ingress‑маршрутизатора. Сервис Redis разделён на primary и реплики: один экземпляр `redis` (primary) и два экземпляра `redis-replica` для асинхронного чтения. Сетевое взаимодействие реализовано через overlay‑сеть, что необходимо для межузловой маршрутизации в Swarm.

Развертывание выполняется стандартными средствами Swarm: инициализация кластера, затем запуск стека с использованием compose‑файла. Важно, что `deploy.replicas` учитывается только при запуске в режиме Swarm.

## Методика нагрузочного тестирования

Для нагрузочного тестирования использован инструмент `wrk`, предназначенный для оценки пропускной способности HTTP‑сервисов и распределения задержек. Тестировался endpoint `POST /api/counter/increment`, поскольку он отражает полный путь обработки запроса (сеть → Flask → Redis → ответ). `wrk` запускался в контейнере, чтобы исключить зависимость от локальной установки.

1. Два режима: некластеризованный (1 реплика `app`) и кластеризованный (4 реплики `app`).
2. Параметры нагрузки: 4 потока, 64 соединения, длительность 60 секунд, сбор статистики задержек с помощью `--latency`.
3. Метрики: средняя пропускная способность (requests/sec), средняя задержка, перцентили p90 и p99, наличие ошибок.

Тест выполнялся локально на одном узле Swarm при неизменных условиях, что обеспечивает сопоставимость результатов по конфигурациям. Пример запуска:

```
docker run --rm --network host -v "D:/vscode_projects/ITMO/counter-deploy:/data" \
	williamyeh/wrk -t4 -c64 -d60s --latency -s /data/wrk_post.lua \
	http://127.0.0.1/api/counter/increment
```

## Результаты и интерпретация

Результаты нагрузочного тестирования приведены в таблице. Оба прогона выполнены на одном хосте, что исключает влияние сетевой гетерогенности.

| Метрика                              | 1 реплика `app` | 4 реплики `app` | Изменение |
| ------------------------------------ | --------------: | --------------: | --------: |
| Пропускная способность, requests/sec |         3257,36 |         3198,98 |     −1,8% |
| Средняя задержка, мс                 |           19,57 |           19,94 |     +1,9% |
| p90, мс                              |           22,35 |           22,08 |     −1,2% |
| p99, мс                              |           29,48 |           27,78 |     −5,8% |
| Доля ошибок                          |              0% |              0% |    0 п.п. |

Интерпретация: прироста пропускной способности не наблюдается, а различия по задержкам находятся в пределах вариативности локального теста. Основная причина — ограничение на стороне Redis и отсутствие распределённой нагрузки по узлам (все реплики работают на одной машине). При увеличении вычислительной сложности обработчиков или при наличии распределённого кластера эффект масштабирования ожидаемо возрастёт.

## Масштабирование сервиса базы данных и особенности репликации

Количество инстансов Redis увеличено до трёх: один primary и две реплики (см. [docker-compose.yml](docker-compose.yml)). Репликация Redis является асинхронной, что означает возможную рассогласованность данных между primary и репликами на небольших интервалах времени. Поскольку приложение выполняет операции записи, оно должно обращаться к primary; чтение с реплик допустимо только при приемлемости eventual consistency.

Выявленные ограничения и риски:

1. Отсутствие автоматического failover: при отказе primary запись невозможна без ручного переключения.
2. Локальность томов: стандартный том `redis_data` привязан к узлу, что снижает устойчивость при перемещении контейнера.
3. Единая точка записи: даже при наличии реплик пропускная способность записи ограничена primary.

Способы устранения или смягчения:

1. Использование Redis Sentinel или Redis Cluster для автоматического failover и управления ролью узлов.
2. Подключение внешних хранилищ (например, CSI‑драйверы, сетевые тома) для обеспечения переносимости данных между узлами.
3. Разделение потоков чтения/записи на уровне приложения (read‑replicas), либо введение шардирования при росте нагрузки на запись.

## Альтернативный подход: Kubernetes (k3s)

При переходе от Swarm к Kubernetes изменяется модель управления: вместо декларативного `docker stack` используется набор манифестов (Deployment, Service, StatefulSet), а оркестрация опирается на control plane, планировщик и объектную модель API‑сервера. Kubernetes предоставляет более развитые механизмы управления состоянием и сетевыми политиками, однако цена — в более высокой сложности администрирования и необходимости обслуживать control plane.

В Swarm механизм балансировки встроен в ingress‑routing и работает автоматически для сервисов, тогда как в Kubernetes требуется отдельный объект Service, а для внешнего доступа — Ingress или LoadBalancer. В части хранилищ Swarm опирается на драйверы томов без строгой объектной модели, тогда как Kubernetes использует PersistentVolume/Claim, что делает состояние более управляемым, но требует корректной настройки класса хранения.

Пример манифеста для k3s приведён в [k3s-deploy.yaml](k3s-deploy.yaml). Он содержит:

1. Deployment с 4 репликами приложения;
2. StatefulSet для primary Redis с томом;
3. Deployment для реплик Redis;
4. Service‑объекты для маршрутизации трафика.

## Выводы

Выполненное тестирование показало, что при локальном запуске и доминирующей роли Redis увеличение числа реплик приложения в Docker Swarm не приводит к росту пропускной способности. Репликация Redis повышает доступность чтений, но не устраняет риски, связанные с единым primary и отсутствием автоматического failover. Kubernetes (k3s) предлагает более развитую модель управления состоянием и сервисами, однако требует более сложной инфраструктуры по сравнению со Swarm. Для промышленной эксплуатации целесообразно дополнять архитектуру механизмами failover и внешним устойчивым хранилищем.
