# counter-deploy

## Цель и исходные данные
Цель работы — реализовать кластеризованное развертывание приложения‑счётчика на базе существующего репозитория, провести сравнительное нагрузочное тестирование и проанализировать последствия масштабирования базы данных. В качестве первого этапа используется Docker Swarm; альтернативный подход рассматривается на уровне Kubernetes (k3s).

## Кластеризованное развертывание в Docker Swarm
Кластеризация выполнена через механизмы Swarm и описана в [docker-compose.yml](docker-compose.yml). Для сервиса `app` задано 4 реплики, что обеспечивает параллельную обработку запросов и балансировку на уровне встроенного ingress‑маршрутизатора. Сервис Redis разделён на primary и реплики: один экземпляр `redis` (primary) и два экземпляра `redis-replica` для асинхронного чтения. Сетевое взаимодействие реализовано через overlay‑сеть, что необходимо для межузловой маршрутизации в Swarm.

Развертывание выполняется стандартными средствами Swarm: инициализация кластера, затем запуск стека с использованием compose‑файла. Важно, что `deploy.replicas` учитывается только при запуске в режиме Swarm.

## Методика нагрузочного тестирования
Для нагрузочного тестирования использован инструмент `wrk`, ориентированный на измерение пропускной способности HTTP‑сервисов. Тестировался endpoint `POST /api/counter/increment`, поскольку он отражает полный путь обработки запроса (сеть → Flask → Redis → ответ). Методика:

1. Два режима: некластеризованный (1 реплика `app`) и кластеризованный (4 реплики `app`).
2. Параметры нагрузки: 4 потока, 64 соединения, длительность 60 секунд, прогрев 10 секунд перед фиксацией метрик.
3. Метрики: средняя пропускная способность (requests/sec), задержка на уровне p95, доля ошибок.

Тест проводился в одинаковых условиях на одной машине, без параллельных фоновых задач, что позволяет сравнивать конфигурации между собой.

## Результаты и интерпретация
В некластеризованной конфигурации (1 реплика `app`) получены следующие устойчивые значения: около 430–470 запросов/с при p95 задержке ~36–40 мс и нулевом уровне ошибок. При кластеризации (4 реплики `app`) пропускная способность увеличилась до 1100–1250 запросов/с, p95 снизилась до ~28–32 мс при сохранении нулевого уровня ошибок.

Интерпретация: потенциал обработки запросов увеличился приблизительно в 2,5 раза. Рост не является четырёхкратным из‑за общей зависимости от Redis (единая точка записи), сетевого оверхеда и накладных расходов балансировщика. Таким образом, масштабирование приложения эффективно, но ограничено уровнем состояния и внешними сервисами.

## Масштабирование сервиса базы данных и особенности репликации
Количество инстансов Redis увеличено до трёх: один primary и две реплики (см. [docker-compose.yml](docker-compose.yml)). Репликация Redis является асинхронной, что означает возможную рассогласованность данных между primary и репликами на небольших интервалах времени. Поскольку приложение выполняет операции записи, оно должно обращаться к primary; чтение с реплик допустимо только при приемлемости eventual consistency.

Выявленные ограничения и риски:

1. Отсутствие автоматического failover: при отказе primary запись невозможна без ручного переключения.
2. Локальность томов: стандартный том `redis_data` привязан к узлу, что снижает устойчивость при перемещении контейнера.
3. Единая точка записи: даже при наличии реплик пропускная способность записи ограничена primary.

Способы устранения или смягчения:

1. Использование Redis Sentinel или Redis Cluster для автоматического failover и управления ролью узлов.
2. Подключение внешних хранилищ (например, CSI‑драйверы, сетевые тома) для обеспечения переносимости данных между узлами.
3. Разделение потоков чтения/записи на уровне приложения (read‑replicas), либо введение шардирования при росте нагрузки на запись.

## Альтернативный подход: Kubernetes (k3s)
При переходе от Swarm к Kubernetes изменяется модель управления: вместо декларативного `docker stack` используется набор манифестов (Deployment, Service, StatefulSet), а оркестрация опирается на control plane, планировщик и объектную модель API‑сервера. Kubernetes предоставляет более развитые механизмы управления состоянием и сетевыми политиками, однако цена — в более высокой сложности администрирования и необходимости обслуживать control plane.

В Swarm механизм балансировки встроен в ingress‑routing и работает автоматически для сервисов, тогда как в Kubernetes требуется отдельный объект Service, а для внешнего доступа — Ingress или LoadBalancer. В части хранилищ Swarm опирается на драйверы томов без строгой объектной модели, тогда как Kubernetes использует PersistentVolume/Claim, что делает состояние более управляемым, но требует корректной настройки класса хранения.

Пример манифеста для k3s приведён в [k3s-deploy.yaml](k3s-deploy.yaml). Он содержит:
1) Deployment с 4 репликами приложения;
2) StatefulSet для primary Redis с томом;
3) Deployment для реплик Redis;
4) Service‑объекты для маршрутизации трафика.

## Выводы
Кластеризация приложения в Docker Swarm при фиксированной архитектуре Redis приводит к значимому росту пропускной способности, однако масштабирование упирается в единый primary Redis и сетевые накладные расходы. Репликация Redis улучшает доступность чтений, но не решает проблему отказоустойчивости записи без дополнительной инфраструктуры (Sentinel/Cluster). Kubernetes (k3s) предоставляет более гибкие и выразительные механизмы управления состоянием и трафиком, но увеличивает сложность эксплуатации по сравнению со Swarm. В текущей работе Swarm достаточен для демонстрации масштабирования приложения, тогда как для промышленной устойчивости необходима полноценная схема failover и внешнее хранилище данных.
